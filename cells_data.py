import pandas as pd
import numpy as np
import os
import re
import json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


def clean_column_name(col):
    """
    æ¸…ç†åˆ—åï¼Œå°†ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    """
    if col == 'Label' or col.startswith('Label_'):
        return col

    col = re.sub(r'[.\s/\-\(\)]', '_', col)
    col = re.sub(r'_+', '_', col)
    col = col.strip('_')

    return col


def convert_label_to_onehot(df, num_classes):
    """
    å°†å•åˆ—Labelè½¬æ¢ä¸ºone-hotç¼–ç çš„å¤šåˆ—

    Parameters:
    -----------
    df : DataFrame
        åŒ…å«Labelåˆ—çš„æ•°æ®æ¡†
    num_classes : int
        ç±»åˆ«æ•°é‡

    Returns:
    --------
    df_onehot : DataFrame
        Labelåˆ—æ›¿æ¢ä¸ºLabel_0, Label_1, ..., Label_nçš„æ•°æ®æ¡†
    """

    print(f"\nå°†Labelè½¬æ¢ä¸ºone-hotç¼–ç ï¼ˆ{num_classes}ä¸ªç±»åˆ«ï¼‰...")

    # æå–Labelå€¼
    label_values = df['Label'].values.astype(int)

    # åˆ›å»ºone-hotç¼–ç 
    one_hot = np.zeros((len(df), num_classes))
    one_hot[np.arange(len(df)), label_values] = 1

    # å¤åˆ¶æ•°æ®æ¡†å¹¶åˆ é™¤åŸå§‹Labelåˆ—
    df_onehot = df.drop(columns=['Label']).copy()

    # æ·»åŠ one-hotç¼–ç åˆ—
    for i in range(num_classes):
        df_onehot[f'Label_{i}'] = one_hot[:, i].astype(int)

    print(f"âœ“ Labelåˆ—å·²è½¬æ¢:")
    print(f"  åŸå§‹: Label (å•åˆ—ï¼Œå€¼ä¸º0-{num_classes - 1})")
    print(f"  è½¬æ¢å: Label_0, Label_1, ..., Label_{num_classes - 1} (one-hotç¼–ç )")

    # æ˜¾ç¤ºç¤ºä¾‹
    print(f"\nç¤ºä¾‹ï¼ˆå‰3è¡Œï¼‰:")
    print(f"  åŸå§‹Label: {label_values[:3]}")
    for i in range(num_classes):
        print(f"  Label_{i}: {one_hot[:3, i]}")

    return df_onehot


def encode_categorical_features(train_df, test_df, exclude_columns):
    """
    å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œæ ‡ç­¾ç¼–ç 

    Parameters:
    -----------
    train_df : DataFrame
        è®­ç»ƒé›†
    test_df : DataFrame
        æµ‹è¯•é›†
    exclude_columns : list
        ä¸éœ€è¦ç¼–ç çš„åˆ—

    Returns:
    --------
    train_encoded, test_encoded, encoders_dict, categorical_columns
    """
    print("\n" + "=" * 70)
    print("æ­¥éª¤: å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œç¼–ç ")
    print("=" * 70)

    train_encoded = train_df.copy()
    test_encoded = test_df.copy()
    encoders = {}
    categorical_columns = []

    for col in train_df.columns:
        if col in exclude_columns:
            continue

        # æ£€æŸ¥æ˜¯å¦æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ˆåˆ†ç±»å˜é‡ï¼‰
        if train_df[col].dtype == 'object' or train_df[col].dtype.name == 'category':
            categorical_columns.append(col)

            print(f"\nç¼–ç åˆ—: {col}")
            print(f"  å”¯ä¸€å€¼æ•°é‡: {train_df[col].nunique()}")

            # åˆ›å»ºæ ‡ç­¾ç¼–ç å™¨
            le = LabelEncoder()

            # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ‰€æœ‰ç±»åˆ«
            all_categories = pd.concat([train_df[col], test_df[col]]).unique()
            le.fit(all_categories)

            # ç¼–ç 
            train_encoded[col] = le.transform(train_df[col])
            test_encoded[col] = le.transform(test_df[col])

            # ä¿å­˜ç¼–ç å™¨
            encoders[col] = {
                'encoder': le,
                'classes': le.classes_.tolist(),
                'num_categories': len(le.classes_)
            }

            print(f"  ç¼–ç èŒƒå›´: 0 - {len(le.classes_) - 1}")
            print(f"  ç¤ºä¾‹: {train_df[col].iloc[0]} -> {train_encoded[col].iloc[0]}")

    if not categorical_columns:
        print("\nâœ“ æ²¡æœ‰éœ€è¦ç¼–ç çš„åˆ†ç±»ç‰¹å¾")
    else:
        print(f"\nâœ“ å…±ç¼–ç äº† {len(categorical_columns)} ä¸ªåˆ†ç±»ç‰¹å¾")

    return train_encoded, test_encoded, encoders, categorical_columns


def save_encoders(encoders, filepath):
    """
    ä¿å­˜ç¼–ç å™¨ä¿¡æ¯åˆ°JSONæ–‡ä»¶
    """
    encoders_info = {}
    for col, info in encoders.items():
        encoders_info[col] = {
            'classes': info['classes'],
            'num_categories': info['num_categories']
        }

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(encoders_info, f, indent=2, ensure_ascii=False)

    print(f"âœ“ ç¼–ç å™¨ä¿¡æ¯å·²ä¿å­˜: {filepath}")


def process_ad_metadata_with_split(input_file, test_size=0.2, random_state=42):
    """
    å¤„ç†ADç»†èƒå…ƒæ•°æ®ï¼Œåˆ›å»ºLabelåˆ—å¹¶åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†

    æ–°å¢åŠŸèƒ½ï¼š
    - å¯¹æ‰€æœ‰åˆ†ç±»ç‰¹å¾è¿›è¡Œæ ‡ç­¾ç¼–ç 
    - å°†Labelè½¬æ¢ä¸ºone-hotç¼–ç 
    - å½»åº•åˆ é™¤æ‰€æœ‰å¯èƒ½å¯¼è‡´æ•°æ®æ³„éœ²çš„åˆ—
    - ä¿å­˜ç¼–ç å™¨æ˜ å°„
    """

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("data", exist_ok=True)

    print("=" * 70)
    print("æ­¥éª¤1: è¯»å–æ•°æ®")
    print("=" * 70)
    df = pd.read_csv(input_file)

    print(f"âœ“ æ•°æ®ç»´åº¦: {df.shape}")
    print(f"âœ“ æ ·æœ¬æ•°: {len(df)}")
    print(f"âœ“ ç‰¹å¾æ•°: {len(df.columns)}")

    print("\n" + "=" * 70)
    print("æ­¥éª¤2: æ¸…ç†åˆ—åï¼ˆå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰")
    print("=" * 70)

    column_mapping = {}
    changed_columns = []

    for old_col in df.columns:
        new_col = clean_column_name(old_col)
        column_mapping[old_col] = new_col
        if old_col != new_col:
            changed_columns.append((old_col, new_col))
            print(f"  {old_col} â†’ {new_col}")

    if not changed_columns:
        print("  âœ“ æ²¡æœ‰éœ€è¦ä¿®æ”¹çš„åˆ—å")
    else:
        print(f"\n  âœ“ ä¿®æ”¹äº† {len(changed_columns)} ä¸ªåˆ—å")

    df.columns = [column_mapping[col] for col in df.columns]

    print("\n" + "=" * 70)
    print("æ­¥éª¤3: åˆ›å»ºLabelåˆ—")
    print("=" * 70)

    if 'Braak' not in df.columns:
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°Braakåˆ—ï¼")
        print(f"å¯ç”¨åˆ—å: {df.columns.tolist()}")
        return None

    braak_to_numeric = {
        '0': 0,
        'I': 1,
        'II': 2,
        #'III': 3,
        #'IV': 4,
        #'V': 5,
        'VI': 3
    }

    df['Label'] = df['Braak'].map(braak_to_numeric)

    print(f"Braak stageå”¯ä¸€å€¼: {sorted(df['Braak'].unique())}")
    print(f"\nLabelåˆ†å¸ƒ:")
    print(df['Label'].value_counts().sort_index())

    # ç¡®å®šç±»åˆ«æ•°
    num_label_classes = int(df['Label'].nunique())
    print(f"\nâœ“ Labelç±»åˆ«æ•°: {num_label_classes}")

    if df['Label'].isna().sum() > 0:
        print(f"\nâš ï¸  è­¦å‘Š: æœ‰{df['Label'].isna().sum()}è¡Œæ— æ³•è½¬æ¢Labelï¼Œå°†åˆ é™¤")
        df = df.dropna(subset=['Label'])
        print(f"âœ“ åˆ é™¤åæ•°æ®ç»´åº¦: {df.shape}")

    print("\n" + "=" * 70)
    print("æ­¥éª¤4: åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†")
    print("=" * 70)

    train_df, test_df = train_test_split(
        df,
        test_size=test_size,
        random_state=random_state,
        stratify=df['Label']
    )

    print(f"âœ“ è®­ç»ƒé›†ç»´åº¦: {train_df.shape}")
    print(f"âœ“ æµ‹è¯•é›†ç»´åº¦: {test_df.shape}")

    print("\n" + "=" * 70)
    print("æ­¥éª¤5: åˆ é™¤ADç›¸å…³åˆ—å’Œå¯èƒ½æ³„éœ²çš„åˆ—")
    print("=" * 70)

    # ğŸ”¥ è¦åˆ é™¤çš„åˆ—ï¼ˆå®Œæ•´åˆ—è¡¨ï¼‰- è¿™æ˜¯å…³é”®ä¿®æ”¹ï¼
    columns_to_remove = [
        # ADç›¸å…³åˆ—ï¼ˆç›´æ¥å…³è”è¯Šæ–­ï¼‰
        'Braak',
        'Amyloid',
        'disease',
        'disease_ontology_term_id',

        # ç»†èƒç±»å‹ç›¸å…³ï¼ˆå¯èƒ½æ³„éœ²ç—…ç†ä¿¡æ¯ï¼‰


        # IDåˆ—ï¼ˆå¯èƒ½ç¼–ç æ‚£è€…æˆ–æ‰¹æ¬¡ä¿¡æ¯ï¼‰
        'donor_id',
        'Donor_id',
        'donor',
        'observation_joinid',
        'cell_id',
        'Sample_ID',
        'Sample_id',
        'sample_id',
        'Sample.ID',

        # ç»„ç»‡ç›¸å…³ï¼ˆå¯èƒ½å…³è”ç—…ç†é˜¶æ®µï¼‰
        # å…¶ä»–å¯ç–‘åˆ—
        'is_primary_data',  # å…ƒæ•°æ®
        'development_stage',
        'development_stage_ontology_term_id',
        'percent_mt',

        'PMI',
        'RIN',
        'Age',
        'suspension_type',
        'self_reported_ethnicity',
        'self_reported_ethnicity_ontology_term_id',
        'SORT',
        'UMAP1',
        'UMAP2',
    ]

    existing_columns_to_remove = [col for col in columns_to_remove if col in df.columns]

    print(f"å°†è¦åˆ é™¤çš„åˆ— ({len(existing_columns_to_remove)}):")
    for col in existing_columns_to_remove:
        print(f"  - {col}")

    train_df_reduced = train_df.drop(columns=existing_columns_to_remove, errors='ignore')
    test_df_reduced = test_df.drop(columns=existing_columns_to_remove, errors='ignore')

    print(f"\nâœ“ åˆ é™¤åè®­ç»ƒé›†ç»´åº¦: {train_df_reduced.shape}")
    print(f"âœ“ åˆ é™¤åæµ‹è¯•é›†ç»´åº¦: {test_df_reduced.shape}")

    # æ˜¾ç¤ºä¿ç•™çš„åˆ—
    print(f"\nä¿ç•™çš„åˆ— ({len(train_df_reduced.columns) - 1}): ")  # -1 for Label
    retained_cols = [c for c in train_df_reduced.columns if c != 'Label']
    for col in retained_cols:
        print(f"  - {col}")

    # ============= ç¼–ç åˆ†ç±»ç‰¹å¾ =============
    print("\n" + "=" * 70)
    print("æ­¥éª¤6: ç¼–ç åˆ†ç±»ç‰¹å¾")
    print("=" * 70)

    exclude_for_encoding = ['Label']

    train_encoded, test_encoded, encoders, categorical_cols = encode_categorical_features(
        train_df_reduced,
        test_df_reduced,
        exclude_for_encoding
    )

    # ä¿å­˜ç¼–ç å™¨ä¿¡æ¯
    if encoders:
        save_encoders(encoders, "data/categorical_encoders.json")

    # ============= å°†Labelè½¬æ¢ä¸ºone-hotç¼–ç  =============
    print("\n" + "=" * 70)
    print("æ­¥éª¤7: å°†Labelè½¬æ¢ä¸ºone-hotç¼–ç ")
    print("=" * 70)

    train_onehot = convert_label_to_onehot(train_encoded, num_label_classes)
    test_onehot = convert_label_to_onehot(test_encoded, num_label_classes)

    print("\n" + "=" * 70)
    print("æ­¥éª¤8: ä¿å­˜æ–‡ä»¶")
    print("=" * 70)

    # ä¿å­˜one-hotç¼–ç åçš„æ–‡ä»¶ï¼ˆç”¨äºè®­ç»ƒï¼‰
    output_train_encoded = "data/train_data_X.csv"
    train_onehot.to_csv(output_train_encoded, index=False)
    print(f"âœ“ è®­ç»ƒé›†ï¼ˆone-hotç¼–ç ï¼‰: {output_train_encoded}")
    print(f"    ç»´åº¦: {train_onehot.shape}")
    print(f"    åˆ—: {train_onehot.columns.tolist()}")

    output_test_encoded = "data/test_data_X.csv"
    test_onehot.to_csv(output_test_encoded, index=False)
    print(f"\nâœ“ æµ‹è¯•é›†ï¼ˆone-hotç¼–ç ï¼‰: {output_test_encoded}")
    print(f"    ç»´åº¦: {test_onehot.shape}")

    # ä¿å­˜åŸå§‹Labelç‰ˆæœ¬ï¼ˆæœªone-hotï¼Œç”¨äºåˆ†æï¼‰
    output_train_original = "data/train_data_original.csv"
    train_encoded.to_csv(output_train_original, index=False)
    print(f"\nâœ“ è®­ç»ƒé›†ï¼ˆåŸå§‹Labelï¼‰: {output_train_original}")

    output_test_original = "data/test_data_original.csv"
    test_encoded.to_csv(output_test_original, index=False)
    print(f"âœ“ æµ‹è¯•é›†ï¼ˆåŸå§‹Labelï¼‰: {output_test_original}")

    # ä¿å­˜å®Œæ•´ç‰ˆï¼ˆåŒ…å«åˆ é™¤çš„åˆ—ï¼Œç”¨äºè°ƒè¯•ï¼‰
    output_train_full = "data/train_data_full.csv"
    train_df.to_csv(output_train_full, index=False)
    print(f"\nâœ“ è®­ç»ƒé›†ï¼ˆå®Œæ•´ç‰ˆï¼ŒåŒ…å«æ‰€æœ‰åˆ—ï¼‰: {output_train_full}")

    output_test_full = "data/test_data_full.csv"
    test_df.to_csv(output_test_full, index=False)
    print(f"âœ“ æµ‹è¯•é›†ï¼ˆå®Œæ•´ç‰ˆï¼ŒåŒ…å«æ‰€æœ‰åˆ—ï¼‰: {output_test_full}")

    print("\n" + "=" * 70)
    print("æ­¥éª¤9: ç”ŸæˆTOMLé…ç½®æ–‡ä»¶")
    print("=" * 70)

    generate_toml_config(train_onehot, encoders, categorical_cols, num_label_classes, "stage_2.toml")

    print("\n" + "=" * 70)
    print("å¤„ç†å®Œæˆï¼")
    print("=" * 70)

    print("\nã€æ•°æ®ç»Ÿè®¡ã€‘")
    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"è®­ç»ƒé›†: {len(train_df)} ({len(train_df) / len(df) * 100:.1f}%)")
    print(f"æµ‹è¯•é›†: {len(test_df)} ({len(test_df) / len(df) * 100:.1f}%)")
    print(f"\nç‰¹å¾ç»Ÿè®¡:")
    num_features_without_label = train_onehot.shape[1] - num_label_classes
    print(f"  æ•°å€¼å‹ç‰¹å¾: {len([c for c in train_encoded.columns if c not in categorical_cols and c != 'Label'])}")
    print(f"  åˆ†ç±»ç‰¹å¾ï¼ˆå·²ç¼–ç ï¼‰: {len(categorical_cols)}")
    print(f"  æ€»ç‰¹å¾æ•°: {num_features_without_label}")
    print(f"  Labelåˆ—æ•°ï¼ˆone-hotï¼‰: {num_label_classes}")

    print("\nã€Labelåˆ†å¸ƒã€‘")
    print(f"{'Label':<10} {'è®­ç»ƒé›†':<15} {'æµ‹è¯•é›†':<15} {'æ€»è®¡':<15}")
    print("-" * 60)
    for label in sorted(df['Label'].unique()):
        train_count = (train_encoded['Label'] == label).sum()
        test_count = (test_encoded['Label'] == label).sum()
        total_count = (df['Label'] == label).sum()
        print(f"{int(label):<10} {train_count:<15} {test_count:<15} {total_count:<15}")

    print("\nã€ç”Ÿæˆçš„æ–‡ä»¶ã€‘")
    print(f"1. {output_train_encoded} - è®­ç»ƒé›†ï¼ˆæ¸…ç†+ç¼–ç ï¼Œç”¨äºè®­ç»ƒï¼‰â­")
    print(f"2. {output_test_encoded} - æµ‹è¯•é›†ï¼ˆæ¸…ç†+ç¼–ç ï¼Œç”¨äºæµ‹è¯•ï¼‰â­")
    print(f"3. {output_train_original} - è®­ç»ƒé›†ï¼ˆæ¸…ç†+åŸå§‹Labelï¼‰")
    print(f"4. {output_test_original} - æµ‹è¯•é›†ï¼ˆæ¸…ç†+åŸå§‹Labelï¼‰")
    print(f"5. {output_train_full} - è®­ç»ƒé›†å®Œæ•´ç‰ˆï¼ˆè°ƒè¯•ç”¨ï¼‰")
    print(f"6. {output_test_full} - æµ‹è¯•é›†å®Œæ•´ç‰ˆï¼ˆè°ƒè¯•ç”¨ï¼‰")
    print(f"7. stage_2.toml - æ¨¡å‹é…ç½®æ–‡ä»¶ â­")
    if encoders:
        print(f"8. data/categorical_encoders.json - åˆ†ç±»ç‰¹å¾ç¼–ç æ˜ å°„")

    print("\nã€ä¸‹ä¸€æ­¥ã€‘")
    print("1. æ£€æŸ¥ train_data_X.csv ç¡®è®¤åˆ—å·²æ­£ç¡®åˆ é™¤")
    print("2. è¿è¡Œè®­ç»ƒï¼šbash dev/train.sh")

    return train_onehot, test_onehot, encoders, num_label_classes


def generate_toml_config(df, encoders, categorical_cols, num_label_classes, output_toml):
    """
    æ ¹æ®one-hotç¼–ç åçš„æ•°æ®æ¡†ç”ŸæˆTOMLé…ç½®æ–‡ä»¶
    """
    # Labelåˆ—ç°åœ¨æ˜¯Label_0, Label_1, ...
    label_columns = [f'Label_{i}' for i in range(num_label_classes)]

    numerical_features = []
    categorical_features = []

    for col in df.columns:
        if col in label_columns:
            continue

        if col in categorical_cols:
            # ä½¿ç”¨ç¼–ç å™¨ä¸­ä¿å­˜çš„ç±»åˆ«æ•°
            num_cats = encoders[col]['num_categories']
            categorical_features.append((col, num_cats))
        else:
            numerical_features.append(col)

    with open(output_toml, 'w', encoding='utf-8') as f:
        f.write("# AD (Alzheimer's Disease) Model Configuration\n")
        f.write("# Auto-generated with one-hot encoded labels\n")
        f.write("# All suspicious columns removed to prevent data leakage\n\n")

        # Labelé…ç½® - æ¯ä¸ªone-hotåˆ—éƒ½æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»
        f.write("[label]\n")
        for i in range(num_label_classes):
            f.write(f"    [label.Label_{i}]\n")
            f.write("    type = \"categorical\"\n")
            f.write(f"    num_categories = 2  # Binary: 0 or 1\n")
            if i < num_label_classes - 1:
                f.write("\n")
        f.write("\n")

        # Dataé…ç½® - ğŸ”¥ ä¸è®¾ç½®id_columnï¼Œé¿å…æ¡†æ¶æŒ‰IDåˆ†ç»„
        f.write("[data]\n")
        f.write("train_csv = \"data/train_data_X.csv\"\n")
        f.write("val_csv = \"data/test_data_X.csv\"\n")
        f.write("# id_column not set - treating each row as independent sample\n\n")

        # æ•°å€¼å‹ç‰¹å¾
        f.write(f"# ==================== Numerical Features ({len(numerical_features)}) ====================\n")
        for feat in sorted(numerical_features):
            f.write(f'[feature.{feat}]\n')
            f.write('type = "numerical"\n')
            f.write('shape = [ 1,]\n\n')

        # åˆ†ç±»ç‰¹å¾ï¼ˆå·²ç¼–ç ï¼‰
        if categorical_features:
            f.write(
                f"# ==================== Categorical Features (Encoded) ({len(categorical_features)}) ====================\n")
            for feat, num_cats in sorted(categorical_features):
                f.write(f'[feature.{feat}]  # Encoded: 0-{num_cats - 1}\n')
                f.write('type = "categorical"\n')
                f.write(f'num_categories = {num_cats}\n\n')

    print(f"âœ“ TOMLé…ç½®å·²ç”Ÿæˆ: {output_toml}")
    print(f"  - Label: {num_label_classes}ä¸ªone-hotåˆ— (Label_0 åˆ° Label_{num_label_classes - 1})")
    print(f"  - æ•°å€¼å‹ç‰¹å¾: {len(numerical_features)}")
    print(f"  - åˆ†ç±»ç‰¹å¾: {len(categorical_features)}")
    print(f"  - IDåˆ—: æœªè®¾ç½®ï¼ˆé¿å…æŒ‰IDåˆ†ç»„ï¼‰")


if __name__ == "__main__":
    input_file = "cells_metadata.csv"

    print("\n" + "=" * 70)
    print(" ADç»†èƒå…ƒæ•°æ®å¤„ç†è„šæœ¬ (å½»åº•æ¸…ç†ç‰ˆ)")
    print("=" * 70)
    print(f"\nè¾“å…¥æ–‡ä»¶: {input_file}\n")

    try:
        result = process_ad_metadata_with_split(
            input_file,
            test_size=0.2,
            random_state=42
        )

        if result is not None:
            print("\n" + "=" * 70)
            print("âœ… æ‰€æœ‰æ–‡ä»¶åˆ›å»ºæˆåŠŸï¼")
            print("=" * 70)

    except FileNotFoundError:
        print(f"\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ '{input_file}'")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback

        traceback.print_exc()